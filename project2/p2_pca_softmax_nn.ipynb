{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.sparse\n",
    "import scipy.optimize\n",
    "import urllib\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Principal Component Analysis [35%]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.1 - Implement PCA [15%]\n",
    "\n",
    "### Restrictions\n",
    "\n",
    "The use of `sklearn.decomposition.PCA` or `np.cov` is prohibited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca(X: np.ndarray, K: int) -> Tuple[np.ndarray, np.ndarray,  np.ndarray]:\n",
    "    \"\"\"\n",
    "    X is an N*D matrix of data (N points in D dimensions)\n",
    "    K is the desired maximum target dimensionality (K <= min{N,D})\n",
    "\n",
    "    should return a tuple (P, Z, evals)\n",
    "    \n",
    "    where P is the projected data (N*K) where\n",
    "    the first dimension is the higest variance,\n",
    "    the second dimension is the second higest variance, etc.\n",
    "\n",
    "    Z is the projection matrix (D*K) that projects the data into\n",
    "    the low dimensional space (i.e., P = X * Z).\n",
    "\n",
    "    and evals, a K dimensional array of eigenvalues (sorted)\n",
    "    \"\"\"\n",
    "    \n",
    "    N, D = X.shape\n",
    "\n",
    "    # make sure we don't look for too many eigs!\n",
    "    if K > N:\n",
    "        K = N\n",
    "    if K > D:\n",
    "        K = D\n",
    "\n",
    "    ### TODO: YOUR CODE HERE\n",
    "    raise NotImplementedError\n",
    "\n",
    "    return (P, Z, evals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first test of PCA will be on Gaussian data with a known covariance matrix. First, let's generate some data and see how it looks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = np.array([[3,2],[2,4]])\n",
    "(U,S,VT) = np.linalg.svd(M)\n",
    "D = np.diag(np.sqrt(S))\n",
    "\n",
    "Si = U @ D @ VT\n",
    "x = np.random.randn(1000,2) @ Si\n",
    "plt.plot(x[:,0], x[:,1], 'b.');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see what the sample covariance is!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.cov(x.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the sample covariance of the data is almost exactly the true covariance of the data. If you run this with 100,000 data points (instead of 1,000), you should get something even closer to \n",
    "$\\begin{bmatrix} 3 & 2 \\\\ 2 & 4 \\end{bmatrix}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's run PCA on this data. We basically know what should happen, but let's make sure it happens anyway (still, given the random nature, the numbers won't be exactly the same). We can project the data onto the first eigenvalue and plot it in red, and the second eigenvalue in green. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(P, Z, evals) = pca(x, 2)\n",
    "\n",
    "x0 = np.dot(np.dot(x, Z[:,0]).reshape(1000,1), Z[:,0].reshape(1,2))\n",
    "x1 = np.dot(np.dot(x, Z[:,1]).reshape(1000,1), Z[:,1].reshape(1,2))\n",
    "\n",
    "plt.plot(x[:,0], x[:,1], 'b.', x0[:,0], x0[:,1], 'r.', x1[:,0], x1[:,1], 'g.');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.2 - Visualization of MNIST [5%]\n",
    "\n",
    "Lets work with some [handwritten digits](https://en.wikipedia.org/wiki/MNIST_database). Before we try PCA on them, let's visualize the digits. Specifically, implement the function `draw_digits`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_digits(X: np.ndarray, Y: np.ndarray):\n",
    "    ### TODO: YOUR CODE HERE\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_digits():\n",
    "    X = np.zeros((1000, 784), dtype=float)\n",
    "    Y = np.zeros((1000,), dtype=int)\n",
    "\n",
    "    with open('data/digits') as f:\n",
    "        for idx, line in enumerate(f):\n",
    "            vals = [int(v) for v in line.split()]\n",
    "            Y[idx] = vals[0] % 10\n",
    "            for i in range(len(vals)//2-1):\n",
    "                X[idx, vals[i*2+1]] = float(vals[i*2+2])\n",
    "            X[idx,:] = X[idx,:] / np.linalg.norm(X[idx,:])\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "X, Y = load_digits()\n",
    "draw_digits(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's look at some \"eigendigits.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(P, Z, evals) = pca(X, 784)\n",
    "evals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eventually, the eigenvalues drop to zero (some may be negative due to floating point errors)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 - Plotting Explained Variance [10%]\n",
    "\n",
    "Plot the explained variance of the principal components, with x-axis being the number of principal components, and the y-axis being the percent variance explained. How many eigenvectors do you have to include before you've accounted for 90% of the variance?\n",
    "95%? Label these points on your plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.4 - Visualization of Dimensionality Reduction [5%]\n",
    "\n",
    "Now, let's plot the top 50 eigenvectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_digits(Z.T[:50,:], np.arange(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do these look like digits? Should they? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - Softmax Regression [45%]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this part of the project is to implement Softmax Regression in order to classify the MNIST digit dataset. Softmax Regression is essentially a two-layer neural network where the output layer applies the Softmax cost function, a multiclass generalization of the logistic cost function.\n",
    "\n",
    "In logistic regression, we have a hypothesis function of the form\n",
    "\n",
    "$$\\mathbb{P}[y=1] = \\frac{1}{1 + e^{-wx}}$$\n",
    "\n",
    "where $\\mathbf{w}$ is our weight vector. Like the hyperbolic tangent function, the logistic function is also a sigmoid function with the characteristic 's'-like shape, though it has a range of $(0, 1)$ instead of $(-1, 1)$. Note that this is technically not a classifier since it returns probabilities instead of a predicted class, but it's easy to turn it into a classifier by simply choosing the class with the highest probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since logistic regression is used for binary classification, it is easy to see that:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\mathbb{P}[y=1] &= \\frac{1}{1 + e^{-wx}} \\\\\n",
    "&= \\frac{e^{wx}}{e^{wx} + e^{0 \\cdot x}} \\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "Similarily,\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\mathbb{P}[y=0] &= 1 - \\mathbb{P}[y=1] \\\\\n",
    "&= 1 - \\frac{e^{wx}}{1 + e^{wx}} \\\\\n",
    "&= \\frac{e^{0 \\cdot x}}{e^{wx} + e^{0 \\cdot x}} \\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "From this form it appears that we can assign the vector $w_1 = w$ as the weight vector for class $1$ and $w_0 = \\vec{0}$ as the weight vector for class $0$. This motivates generalization to classification with more than 2 classes. By assigning a separate weight vector $w_i$ to each class, for each example $x$ we can predict the probability that it is class $i$, and again we can classify by choosing the most probable class.\n",
    "\n",
    "$$\\mathbb{P}[y=i] = \\frac{e^{w_i x}}{\\sum\\limits_j e^{w_j x}}$$\n",
    "\n",
    "A more compact way of representing the values $w_i x$ is $Wx$ where each row $i$ of $W$ is $w_i$. We can also represent a dataset $\\{x_i\\}$ with a matrix $X$ where each column is a single example, resulting in $WX$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2.1 - Questions [5%]\n",
    "\n",
    "For both problems, assume there are $C$ classes, $n$ be the number of samples, and $d$ be the number of features for each sample. \n",
    "\n",
    "1. Prove that the probabilities outputed by the softmax function sum to 1. \n",
    "\n",
    "**ANSWER**:\n",
    "\n",
    "2. Given the description of matrices $W$, $X$ above, what are the dimensions of $W$, $X$, and $WX$? \n",
    "\n",
    "**ANSWER**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2.2 - Implementing a Softmax Classifier [20%]\n",
    "We can also train on this model with an appropriate loss function. The Softmax loss function is given by\n",
    "\n",
    "$$\\xi(W) = -\\sum\\limits_{i=1}^{n}\\sum\\limits_{j=1}^{C}\\delta_{y_i,j}\\log \\mathbb{P}(y_i = j)$$\n",
    "\n",
    "where $n$ is the number of samples, $C$ is the number of classes, $\\delta_{y_i,j}$ is the [kronecker delta](https://en.wikipedia.org/wiki/Kronecker_delta), $y_i$ is the label of sample $i$, and $\\mathbb{P}(y_i = j)$ is calculated using a softmax function using the weights from matrix $W$.\n",
    "\n",
    "Note that \n",
    "\n",
    "$$\\delta_{y_i,j} = \\begin{cases}1 &\\text{if } y_i = j \\\\ 0 & \\text{if } y_i \\neq j\\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient of this function is given by \n",
    "\n",
    "$$\\nabla_{\\vec{w_j}} \\xi(W) = \\sum_{i=1}^{n}{ \\left[ x_i \\left( \\mathbb{P}[y_i = j] - \\delta_{y_i, j} \\right) \\right]}$$\n",
    "\n",
    "The derivation for this can be found in the references. Note that the kronecker delta and the probabilities can be represented as matrices, which makes the code for the loss and the gradient very simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxRegression:\n",
    "    \"\"\"\n",
    "    Here you will fill in this incomplete implementation of Softmax regression.\n",
    "\n",
    "    Adapted from code by Jatin Shah\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes, ex_size, opts={'maxIter':400}):\n",
    "        \"\"\"\n",
    "        num_classes:    number of possible classifications\n",
    "        ex_size:        size of attribute array (number of input features)\n",
    "        reg:            regularizing term coefficient (lambda)\n",
    "        opts:           in this class the only option used is maxIter\n",
    "        \"\"\"\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.ex_size = ex_size\n",
    "        self.opts = opts\n",
    "\n",
    "        # Initialize weight matrix with empty matrix\n",
    "        self.W = np.zeros((num_classes, ex_size))\n",
    "\n",
    "\n",
    "    def cost(self, X, Y, W=None):\n",
    "        \"\"\"\n",
    "        Calculate the cost function for X and Y using current weight matrix W. Note that we are not using\n",
    "        a regularizer in the cost; this is equivalent to lambda = 0.\n",
    "\n",
    "        X:              (M x N) matrix of input feature values,\n",
    "                            where M = ex_size, N = number of examples\n",
    "        Y:              (N x 1) array of expected output classes for each example\n",
    "\n",
    "        Returns the cost and its gradient, which is the form needed to use scipy.optimize.minimize\n",
    "        \"\"\"\n",
    "\n",
    "        if W is None:\n",
    "            W = this.W\n",
    "        num_classes = self.num_classes\n",
    "        ex_size = self.ex_size\n",
    "\n",
    "        W = W.reshape(num_classes, ex_size)         # Ensure W is in the correct dimensions\n",
    "        N = X.shape[1]                              # N = number of examples\n",
    "\n",
    "        W_X = W.dot(X)                              # This is our activation matrix with dimensions (A * B)\n",
    "                                                    # where A is the number of classes and B is the number\n",
    "                                                    # of examples. (W_X[a, b] gives the activation of example\n",
    "                                                    # b for class a.) You will use this matrix to find the\n",
    "                                                    # probabilities that example b is class a using the\n",
    "                                                    # softmax formula.\n",
    "\n",
    "        W_X = W_X - np.amax(W_X)\n",
    "\n",
    "        # This is the indicator function used in the loss function, where indicator[a, b] = 1\n",
    "        # when example b is labeled a (according to the target Y) and indicator[a, b] = 0 otherwise.\n",
    "\n",
    "        indicator = scipy.sparse.csr_matrix((np.ones(N), (Y, np.array(range(N)))))\n",
    "        indicator = np.resize(np.array(indicator.todense()), (num_classes, N))\n",
    "\n",
    "\n",
    "        # Each column of W_X is the set of activations for each class corresponding to\n",
    "        # one example; the probabilties are given by the exponential of each entry\n",
    "        # divided by the sum of the exponentials over the entire column.\n",
    "\n",
    "        # The cost associated with a single example is given by -1 times the log probability\n",
    "        # of the true class; initialize the cost variable to the AVERAGE cost over all the examples.\n",
    "        # Hint: there's an easy way to do this with the indicator matrix.\n",
    "\n",
    "        # The gradient has the same dimensions as W, and each component (i,j) represents the\n",
    "        # derivative of the cost with respect to the weight associated with class i, attribute j.\n",
    "        # The gradient associated with a single example x is given by -1 * A * x_T, where x_T is\n",
    "        # the transpose of the example, and A is a vector with component i given by (1 - P(class = i))\n",
    "        # if the true class is i, and (-P(class = i)) otherwise. Notice that this multiplication gives\n",
    "        # the desired dimensions. Find the AVERAGE gradient over all the examples. Again, there is\n",
    "        # an easy way to do this with the indicator matrix.\n",
    "\n",
    "        # TODO: Compute the predicted probabilities, the total cost, and the gradient.\n",
    "\n",
    "        ### YOUR CODE HERE ###\n",
    "\n",
    "        raise NotImplementedError\n",
    "        # probabilities =\n",
    "        # cost =\n",
    "        # gradient =\n",
    "\n",
    "        ### YOUR CODE (ENDS) HERE ###\n",
    "\n",
    "        # flatten is needed by scipy.optimize.minimize\n",
    "        return cost, gradient.flatten()\n",
    "\n",
    "\n",
    "    def train(self, X, Y):\n",
    "        \"\"\"\n",
    "        Train to find optimal weight matrix W. Here we make use of the SciPy optimization library but\n",
    "        in theory you could implement gradient descent to do this as well.\n",
    "\n",
    "        X:              (M x N) matrix of input feature values,\n",
    "                            where M = ex_size, N = number of examples\n",
    "        Y:              (N x 1) array of expected output classes for each example\n",
    "        maxIter:        Maximum training iterations\n",
    "        \"\"\"\n",
    "\n",
    "        num_classes = self.num_classes\n",
    "        ex_size = self.ex_size\n",
    "        W = self.W\n",
    "\n",
    "        # Set maxIter hyperparameter\n",
    "        if self.opts['maxIter'] is None:\n",
    "            self.opts['maxIter'] = 400\n",
    "\n",
    "        # Lambda function needed by scipy.optimize.minimize\n",
    "        J = lambda w: self.cost(X, Y, w)\n",
    "\n",
    "        result = scipy.optimize.minimize(J, W, method='L-BFGS-B', jac=True, options={'maxiter': self.opts['maxIter'], 'disp': True})\n",
    "        self.W = result.x # save the optimal solution found\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Use W to predict the classes of each example in X.\n",
    "\n",
    "        X:              (M x N) matrix of input feature values,\n",
    "                            where M = ex_size, N = number of examples\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        W = self.W.reshape(self.num_classes, self.ex_size)\n",
    "        W_X = W.dot(X)\n",
    "\n",
    "        # TODO: Compute the predicted probabilities and the predicted classes for each example\n",
    "        # Reminder: The predicted class for a single example is just the one with the highest probability\n",
    "\n",
    "        ### YOUR CODE HERE ###\n",
    "\n",
    "        raise NotImplementedError\n",
    "        # probabilities =\n",
    "        # predicted_classes =\n",
    "\n",
    "        ### YOUR CODE (ENDS) HERE ###\n",
    "\n",
    "        return predicted_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can test your implementation of `SoftmaxRegression` by running the following snippet below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def good_job(path):\n",
    "    a = plt.imread(urllib.request.urlopen(path))\n",
    "    fig = plt.imshow(a)\n",
    "    fig.axes.get_xaxis().set_visible(False)\n",
    "    fig.axes.get_yaxis().set_visible(False)\n",
    "    plt.show()\n",
    "\n",
    "def loadMNIST(image_file, label_file):\n",
    "    \"\"\"\n",
    "    returns a 28x28x[number of MNIST images] matrix containing\n",
    "    the raw MNIST images\n",
    "    :param filename: input data file\n",
    "    \"\"\"\n",
    "\n",
    "    with open(image_file, \"r\") as f:\n",
    "        magic = np.fromfile(f, dtype=np.dtype('>i4'), count=1)\n",
    "\n",
    "        num_images = np.fromfile(f, dtype=np.dtype('>i4'), count=1)[0]\n",
    "        num_rows = np.fromfile(f, dtype=np.dtype('>i4'), count=1)[0]\n",
    "        num_cols = np.fromfile(f, dtype=np.dtype('>i4'), count=1)[0]\n",
    "\n",
    "        exSize = num_rows * num_cols\n",
    "        images = np.fromfile(f, dtype=np.ubyte)\n",
    "        images = images.reshape((num_images, exSize)).transpose()\n",
    "        images = images.astype(np.float64) / 255\n",
    "\n",
    "    with open(label_file, 'r') as f:\n",
    "        magic = np.fromfile(f, dtype=np.dtype('>i4'), count=1)\n",
    "        num_labels = np.fromfile(f, dtype=np.dtype('>i4'), count=1)\n",
    "        labels = np.fromfile(f, dtype=np.ubyte)\n",
    "\n",
    "    return images, labels\n",
    "\n",
    "ex_size = 28*28   # size of MNIST digits\n",
    "num_classes = 10  # 10 digits to classify\n",
    "\n",
    "X, Y = loadMNIST('data/train-images.idx3-ubyte', 'data/train-labels.idx1-ubyte')\n",
    "sm = SoftmaxRegression(num_classes, ex_size)\n",
    "sm.train(X, Y)\n",
    "\n",
    "testX, testY = loadMNIST('data/t10k-images.idx3-ubyte', 'data/t10k-labels.idx1-ubyte')\n",
    "predictions = sm.predict(testX)\n",
    "accuracy = 100 * np.sum(predictions == testY, dtype=np.float64) / testY.shape[0]\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "assert accuracy >= 93.99, \"Your implementation is sadly incorrect :(\"\n",
    "\n",
    "print(\"TEST PASSED!!!\")\n",
    "good_job(\"https://www.cs.umd.edu/~tomg/img/important_memes/hacker_database.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2.3 - Stability [10%]\n",
    "\n",
    "In the `cost` function of `SoftmaxRegression`, we see the line\n",
    "\n",
    "```python3\n",
    "W_X = W_X - np.amax(W_X)\n",
    "```\n",
    "\n",
    "1. What is this operation doing?\n",
    "\n",
    "**ANSWER**:\n",
    "\n",
    "2. Show that this does not affect the predicted probabilities.\n",
    "\n",
    "**ANSWER**:\n",
    "\n",
    "3. Why might this be an optimization? Justify your answer.\n",
    "\n",
    "**ANSWER**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 - Analysis of Classifier Accuracy [10%]\n",
    "\n",
    "Plot the accuracy of the classifier as a function of the number of examples seen.\n",
    "Do you observe any overfitting or underfitting? Discuss and expain what you observe. Use the helper functions provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(classifier, X, Y, Xtest, Ytest):\n",
    "    \"\"\"\n",
    "    Train a classifier on data (X,Y) and evaluate on\n",
    "    data (Xtest,Ytest).  Return a tuple of:\n",
    "      * Training data accuracy\n",
    "      * Test data accuracy\n",
    "    \"\"\"\n",
    "    classifier.train(X, Y)\n",
    "    train_acc = np.mean(Y == classifier.predict(X))\n",
    "    test_acc = np.mean(Ytest == classifier.predict(Xtest))\n",
    "\n",
    "    print(f\"Training accuracy {train_acc}, test accuracy {test_acc}\")\n",
    "\n",
    "    return train_acc, test_acc\n",
    "\n",
    "def learning_curve(classifier, X, Y, Xtest, Ytest):\n",
    "    \"\"\"\n",
    "    Generate a learning curve by repeatedly halving the amount of\n",
    "    training data until none is left.\n",
    "\n",
    "    We return a triple containing:\n",
    "      * The sizes of data sets we trained on\n",
    "      * The training accuracies at each level\n",
    "      * The test accuracies at each level\n",
    "    \"\"\"\n",
    "\n",
    "    N = X.shape[1]                   # how many total points?\n",
    "    M = int(np.ceil(np.log2(N)))     # how many classifiers will we have to train?\n",
    "\n",
    "    data_sizes = np.zeros(M)\n",
    "    train_accs  = np.zeros(M)\n",
    "    test_accs   = np.zeros(M)\n",
    "\n",
    "    for i in range(1, M+1):\n",
    "        # select every 2^(M-i)th point\n",
    "        ids = np.arange(0, N, 2**(M-i))\n",
    "        Xtr = X[:, ids]\n",
    "        Ytr = Y[ids]\n",
    "\n",
    "        # report what we're doing\n",
    "        print(f\"Training classifier on {ids.size} points...\")\n",
    "\n",
    "        # train the classifier\n",
    "        train_acc, test_acc = train_test(classifier, Xtr, Ytr, Xtest, Ytest)\n",
    "\n",
    "        # store the results\n",
    "        data_sizes[i-1] = ids.size\n",
    "        train_accs[i-1] = train_acc\n",
    "        test_accs[i-1]  = test_acc\n",
    "\n",
    "    return (data_sizes, train_accs, test_accs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
