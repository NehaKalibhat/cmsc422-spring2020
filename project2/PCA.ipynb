{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import util\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.1 - Implement PCA [15%]\n",
    "\n",
    "### Restrictions\n",
    "\n",
    "The use of `sklearn.decomposition.PCA` or `np.cov` is prohibited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca(X: np.ndarray, K: int) -> Tuple[np.ndarray, np.ndarray,  np.ndarray]:\n",
    "    \"\"\"\n",
    "    X is an N*D matrix of data (N points in D dimensions)\n",
    "    K is the desired maximum target dimensionality (K <= min{N,D})\n",
    "\n",
    "    should return a tuple (P, Z, evals)\n",
    "    \n",
    "    where P is the projected data (N*K) where\n",
    "    the first dimension is the higest variance,\n",
    "    the second dimension is the second higest variance, etc.\n",
    "\n",
    "    Z is the projection matrix (D*K) that projects the data into\n",
    "    the low dimensional space (i.e., P = X * Z).\n",
    "\n",
    "    and evals, a K dimensional array of eigenvalues (sorted)\n",
    "    \"\"\"\n",
    "    \n",
    "    N, D = X.shape\n",
    "\n",
    "    # make sure we don't look for too many eigs!\n",
    "    if K > N:\n",
    "        K = N\n",
    "    if K > D:\n",
    "        K = D\n",
    "\n",
    "    ### TODO: YOUR CODE HERE\n",
    "    raise NotImplementedError\n",
    "\n",
    "    return (P, Z, evals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first test of PCA will be on Gaussian data with a known covariance matrix. First, let's generate some data and see how it looks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = np.array([[3,2],[2,4]])\n",
    "(U,S,VT) = np.linalg.svd(M)\n",
    "D = np.diag(np.sqrt(S))\n",
    "\n",
    "Si = U @ D @ VT\n",
    "x = np.random.randn(1000,2) @ Si\n",
    "plt.plot(x[:,0], x[:,1], 'b.');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see what the sample covariance is!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.cov(x.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the sample covariance of the data is almost exactly the true covariance of the data. If you run this with 100,000 data points (instead of 1,000), you should get something even closer to \n",
    "$\\begin{bmatrix} 3 & 2 \\\\ 2 & 4 \\end{bmatrix}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's run PCA on this data. We basically know what should happen, but let's make sure it happens anyway (still, given the random nature, the numbers won't be exactly the same). We can project the data onto the first eigenvalue and plot it in red, and the second eigenvalue in green. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(P, Z, evals) = pca(x, 2)\n",
    "\n",
    "x0 = np.dot(np.dot(x, Z[:,0]).reshape(1000,1), Z[:,0].reshape(1,2))\n",
    "x1 = np.dot(np.dot(x, Z[:,1]).reshape(1000,1), Z[:,1].reshape(1,2))\n",
    "\n",
    "plt.plot(x[:,0], x[:,1], 'b.', x0[:,0], x0[:,1], 'r.', x1[:,0], x1[:,1], 'g.');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.2 - Visualization of MNIST [5%]\n",
    "\n",
    "Lets work with some [handwritten digits](https://en.wikipedia.org/wiki/MNIST_database). Before we try PCA on them, let's visualize the digits. Specifically, implement the function `draw_digits`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_digits(X: np.ndarray, Y: np.ndarray):\n",
    "    ### TODO: YOUR CODE HERE\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_digits():\n",
    "    X = np.zeros((1000, 784), dtype=float)\n",
    "    Y = np.zeros((1000,), dtype=int)\n",
    "\n",
    "    with open('data/digits') as f:\n",
    "        for idx, line in enumerate(f):\n",
    "            vals = [int(v) for v in line.split()]\n",
    "            Y[idx] = vals[0] % 10\n",
    "            for i in range(len(vals)//2-1):\n",
    "                X[idx, vals[i*2+1]] = float(vals[i*2+2])\n",
    "            X[idx,:] = X[idx,:] / np.linalg.norm(X[idx,:])\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "X, Y = load_digits()\n",
    "draw_digits(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's look at some \"eigendigits.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(P, Z, evals) = pca(X, 784)\n",
    "evals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eventually, the eigenvalues drop to zero (some may be negative due to floating point errors)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 - Plotting Explained Variance [10%]\n",
    "\n",
    "Plot the explained variance of the principal components, with x-axis being the number of principal components, and the y-axis being the percent variance explained. How many eigenvectors do you have to include before you've accounted for 90% of the variance?\n",
    "95%? Label these points on your plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.4 - Visualization of Dimensionality Reduction [5%]\n",
    "\n",
    "Now, let's plot the top 50 eigenvectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_digits(Z.T[:50,:], np.arange(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do these look like digits? Should they? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
